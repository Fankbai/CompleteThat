\documentclass[12pt]{article}
\usepackage{fullpage,graphicx,psfrag,amsmath,amsfonts,verbatim}
\usepackage{listings, color}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\usepackage[scaled]{beramono}
\usepackage{courier, natbib}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{hyperref, adjustbox}
\usepackage[small,bf]{caption}
\usepackage{float}
\usepackage{booktabs}
\usepackage{algpseudocode, algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicuntil}{\textbf{Until:}}
\renewcommand{\algorithmicrepeat}{\textbf{Repeat:}}

\newcommand{\subf}[2]{%
  {\small\begin{tabular}[t]{@{}c@{}}
  #1\\#2
  \end{tabular}}%
}

\lstset{
  language=Python,
  backgroundcolor=\color{white},
  breaklines=true,
  captionpos=b,
  commentstyle=\color{mygreen},
  showstringspaces=false,
  formfeed=newpage,
  keepspaces=true,
  stringstyle=\color{mymauve},
  keywordstyle=\color{blue},
  numbers=left,
  tabsize=4,
  numberstyle=\footnotesize,
  basicstyle=\footnotesize,
  morekeywords={models, lambda, forms, True, False, self}
}

\newcommand{\code}[2] {
  \hrulefill
  \subsection*{#1}
  \lstinputlisting{#2}
  \vspace{2em}
}

\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}

\input defs.tex

\title{\texttt{CompleteThat}: A python package for  low-rank matrix completion\\EEOR E4650 Course Project
}
\author{Joshua Edgerton \and Esteban Fajardo}
\date{January 5, 2014}

\begin{document}

\maketitle

\begin{abstract}
We have developed a Python package (\texttt{CompleteThat}) to perform low rank matrix completion. Given a low rank matrix with partial entries we implemented different methods to solve the matrix completion problem:  First, for ``small'' problems we implemented two memory-based algorithms  following the work by Tanner and Wei~\cite{Tanner:2014} to find, given a number $r$, the matrix with rank $r$ that best fits the data using the Frobenius norm. Second, when the problem does not fit into memory, we implemented a memory-fitting algorithm (stochastic gradient descent) following the approach in Zhang~\cite{zhang:2004} and Bottou~\cite{bottou:2012}. The package is available at \url{https://pypi.python.org/pypi/completethat/0.1dev}. 
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
Matrix factorization has recently seen a large growth in popularity within the mathematics, statistics, and computer science communities as industry continues to apply machine learning techniques on a wide array of problems and scenarios. For our convex optimization project, we decided to take some of the more interesting and applicable topics and algorithms of our class and develop a python package to implement them. We briefly discuss the theory behind matrix factorizing, the models and approaches we choose to use and the algorithms for the optimization. Later, we walk through two case studies illustrating the usefulness and applicability in the context of image processing and a recommendation system for Yahoo music and movie data. 

\section{Matrix Completion Problem}
The matrix completion problem can be formally stated as follows. We are interested in recovering a matrix $M \in \reals^{n_1 \times n_2}$ but only get to observe a number $m \ll n_1n_2$ of its entries. Thus, we want to find a solution to the following optimization problem

\begin{equation}
    \begin{array}{ll}
    \mbox{minimize}   &  \rank(X)\\
    \mbox{subject to} & X_{ij} = M_{ij} \qquad (i,j)\in \Omega
    \end{array}
    \label{eq:original_problem1}
\end{equation}
where $X\in \reals^{n_1 \times n_2}$ is the decision variable and $\rank(X)$ is equal to the rank of the matrix $X$. The problem (\ref{eq:original_problem1}) seeks the simplest matrix fitting the observed data. Of course, if there were only one low-rank object fitting the data, this would recover $M$.  
In order to fix the notation correctly define the projector $P_{\Omega}:\reals^{n_1\times n_2} \rightarrow \reals^{n_1\times n_2} $ as

\begin{equation*}
P_{\Omega}(A) = \left \{ 
	\begin{array}{lr}
		A_{ij} & : (i,j) \in \Omega\\
		0 & : \mathrm{otherwise}
	\end{array}
	\right.
\end{equation*}\\
Using the projector, Problem (\ref{eq:original_problem1}) can be rewritten as
\begin{equation}
    \begin{array}{ll}
    \mbox{minimize}   &  \rank(X)\\
    \mbox{subject to} & P_{\Omega}(X) = P_{\Omega}(M)
    \end{array}
    \label{eq:original_problem2}
\end{equation}

Unfortunately, this optimization problem has been shown to be NP-hard and all know algorithms which provide exact solutions require time doubly exponential in the dimension of the matrix both in theory and in practice~\cite{Candes:2009}. Also, the $\rank(X)$ makes the problem non-convex. 

Several approximations to the problem exists. One, the ``tightest convex relaxation of $\rank(X)$''~\cite{Fazel:2002} is the following problem

\begin{equation}
    \begin{array}{ll}
    \mbox{minimize}   &  \|X\|_* :=\sum_{i=1}^r \sigma_i(X) \\
    \mbox{subject to} & P_{\Omega}(X) = P_{\Omega}(M)
    \end{array}
    \label{eq:nucl_problem}
\end{equation}

where $\sigma_i(X)$ denotes the $i$th largest singular value of $X$ and $\|X\|_*$ is the called the nuclear norm. The main point of this relaxation is that the nuclear norm is a convex function and thus can be optimized efficiently via semidefinite programming or by iterative soft thresholding algorithms~\cite{cai:2010}~\cite{goldfarb:2011}.

Alternative to nuclear norm minimization there have been many algorithms which are designed to target the following optimization problem
\begin{equation}
    \begin{array}{ll}
    \mbox{minimize}_{Y,Z}   & \frac{1}{2} \|P_{\Omega}(YZ) - P_{\Omega}(M)\|^2_F \\
    \end{array}
    \label{eq:frob_problem}
\end{equation}
where $X=YZ$ is the completed matrix and $Y\in \reals^{n_1\times r}, Z\in \reals^{r\times n_2}$ and $r$ represents the (hopefully small) rank of the matrix $X$. The main point of this relaxation is the use of an alternating minimization scheme or Gauss-Seidel or 2-block coordinate descent method, where first $Y$ is fixed and then the problem is reduced to a convex, standard least squares problem on $Z$. Later, $Z$ is fixed and then the problem is reduced to a convex, standard least squares problem on $Y$.

\section{Implementation}
\texttt{CompleteThat} is a python package that solves the low rank matrix completion problem. Given a low rank matrix with partial entries the package solves an optimization problem to estimate the missing entries. We allow the user to choose between several optimization algorithms including two in memory based algorithms, alternating steepest descent and scaled alternating steepest descent, and one out of memory fitting procedure using stochastic gradient descent. We use extensively the numerical libraries \texttt{spicy} and \texttt{numpy}, and the current implementation includes two classes, \texttt{MatrixCompletion} for in memory based algorithms and \texttt{MatrixCompletionBD} for the memory fitting procedure. The package is available worldwide and can be
downloaded from \url{https://pypi.python.org/pypi/completethat/0.1dev}.

\subsection*{CVX}
Problem (\ref{eq:nucl_problem}) can be easily solved directly using CVX, a package for specifying and solving convex programs~\cite{cvx}, ~\cite{gb08}, with the following code:

\begin{verbatim}
index = find(~isnan(M));
cvx_begin
    variable X(size(M));
    minimize norm_nuc(X)
    % s.t.
    X(index) == M(index);
cvx_end
\end{verbatim}

Where M is a MATLAB matrix with nan on the missing entries and \texttt{X(index) == M(index)} corresponds to the $P_{\Omega}(X) = P_{\Omega}(M)$ restriction. However, the computation is very slow and for any matrix greater than 100x100 the computational time is too high. Since this is clearly unsatisfactory for practical purposes, specific algorithms were implemented on the package. 

\subsection*{Algorithms}
\subsubsection*{Low rank matrix completion by alternating steepest descent methods}
The  alternating steepest descent methods implemented on \texttt{CompleteThat} solve the Problem (\ref{eq:frob_problem}) above.  Let the objective function of the optimization problem $f: \reals^{n_1\times r}\times \reals^{r\times n_2} \rightarrow \reals$ be defined as
\begin{equation}
	f(Y,Z):=\frac{1}{2} \|P_{\Omega}(YZ) - P_{\Omega}(M)\|^2_F.
	\label{eq:ASD_prob}
\end{equation}

To solve efficiently the above problem, Tanner and Wei~\cite{Tanner:2014} use a single step of simple line-search along the gradient descent directions.  Let us write $f(Y, Z)$ as $f_Z(Y)$ when $Z$ is held constant and $f_Y(Z)$ when $Y$ is held constant and let $t_y, t_z$ be the steepest descent step sizes for descent directions.

The Alternating Steepest Descent Method (ASD) applies steepest gradient descent to $f(Y, Z)$ in (\ref{eq:ASD_prob}) alternatively with respect to $Y$ and $Z$. It can be shown that the directions of gradient ascent and the steepest descent step sizes are~\cite{Tanner:2014} :
\[
\nabla f_{Z}(Y) = -(P_{\Omega}(M) - P_{\Omega}(Y Z))Z^T\
\]
\[
\nabla f_{Y}(Z) = -Y^T(P_{\Omega}(M)-P_{\Omega}(YZ)).
\]
\[
t_{y} = \frac{\|  \nabla f_{Z}(Y) \|^2_F}{\| P_{\Omega}(\nabla f_{Z} (Y)Z\|^2_F}
\]
\[
 t_{z} = \frac{\|  \nabla f_{Y}(Z) \|^2_F}{\| P_{\Omega}(Y \nabla f_Y (Z)) \|^2_F}
\]

\begin{algorithm}[h]
\caption{Alternating Steepest Descent (ASD)~\cite{Tanner:2014}}
\begin{algorithmic}
\Require $r \in \reals, P_{\Omega}(M), Y_0\in \reals^{n_1\times r}, Z_0\in \reals^{r\times n_2}$
\Repeat
	\State $\nabla f_{Z_i}(Y_i) = -(P_{\Omega}(M) - P_{\Omega}(Y_i Z_i))Z_i^T, t_{y_i} = \frac{\|  \nabla f_{Z_i}(Y_i) \|^2_F}{\| P_{\Omega}(\nabla f_{Z_i} (Y_i)Z_i \|^2_F}$
	\State $Y_{i+1} = Y_i - t_{y_i} f_{Z_i}(X_i)$
	\State $\nabla f_{Y_{i+1}}(Z_i) = -Y^T_{i+1}(P_{\Omega}(M)-P_{\Omega}(Y_{i+1}Z_i)), t_{z_i} = \frac{\|  \nabla f_{Y_{i+1}}(Z_i) \|^2_F}{\| P_{\Omega}(Y_{i+1} \nabla f_Y{i+1} (Z_i)) \|^2_F}$
	\State $Z_{i+1} = Z_i - t_{z_i}\nabla f_{Y_{i+1}}(Z_i)$
	\State $i = i +1$
\Until {termination criteria is not reached}
\Ensure $X:= Y_i Z_i \in \reals^{n_1\times n_2}$
\end{algorithmic}
\end{algorithm}

The Scaled Alternating Steepest Descent Method (sASD) is an accelerated version of ASD and uses a scaled gradient descent direction with exact line-search. Its main motivation for the scaled factors is the explicit expression for the Newton directions for the problem (\ref{eq:ASD_prob}) if all the entries in the matrix $M$ were know:
\[
(M - YZ)Z^T(ZZ^T)^{-1}
\]
\[
(X^TX)^{-1}X^T(M-YZ)
\]
which are the gradient descent directions scaled by $(ZZ^T)^{-1}$ and $(X^TX)^{-1}$.

\begin{algorithm}[h]
\caption{Scaled Alternating Steepest Descent (sASD)~\cite{Tanner:2014}}
\begin{algorithmic}
\Require $r \in \reals, P_{\Omega}(M), Y_0\in \reals^{n_1\times r}, Z_0\in \reals^{r\times n_2}$
\Repeat
	\State $\nabla f_{Z_i}(Y_i) = -(P_{\Omega}(M) - P_{\Omega}(Y_i Z_i))Z_i^T$
	\State $d_{y_i} = -\nabla f_{Z_i} (Y_i)(Z_iZ_i^T)^{-1}, t_{y_i} =\frac{ -\nabla f_{Z_i}(Y_i)\cdot d_{y_i}}{\| P_{\Omega} (d_{y_i}Z_i)\|_F^2}$
	\State $Y_{i+1} = Y_i + t_{y_i}d_{y_i}$
	\State $\nabla f_{Y_{i+1}} (Z_i) = - Y_{i+1}^T(P_{\Omega}(M) - P_{\Omega}(Y_{i+1}Z_i))$
	\State $d_{z_i} = (Y_{i+1}^TY_{i+1}^{-1} \nabla f_{Y_{i+1}}(Z_i), t_{z_i} = \frac{-\nabla f_{Y_{i+1}}(Z_i)\cdot d_{z_i}}{\| P_{\Omega}(Y_{i+1}d_{z_i})\|_F^2}$
	\State $Z_{i+1} = Z_i + t_{z_i} d_{z_i}$
	\State $i = i + 1$
\Until {termination criteria is not reached}
\Ensure $X:= Y_i Z_i \in \reals^{n_1\times n_2}$
\end{algorithmic}
\end{algorithm}

\subsubsection*{Low rank matrix completion by stochastic gradient descent}
The formal problem as defined in the model section is stated in the language of linear algebra and the typical optimizations using gradient methods require access to all of the data at once for updating the parameters, since its dependent on calculating inverse of matrices or solving systems of linear equations. However, given the scale of data that is common in today's digital world, the need for more efficient and memory friendly algorithms arises. With stochastic gradient descent, a variation of the standard batch gradient optimization, instead of calculating the gradient for the user feature vector using all the item feature vectors that the user has rated as would be done in usually, we approximate the user's gradient using the current item vector for the current rating and vice versa for updates of the item vectors. Thus we are essentially exchanging the luxury of using less memory and computational resources for more time and iterations spent with stochastic gradient descent.\footnote{We note that our formulation is not convex but, due to shuffling of the data and the way the gradients are estimated, the algorithm will converge to at least a local minimum.}


\begin{algorithm}[H]
\caption{Stochastic Gradient Descent (SGD)}
\begin{algorithmic}
\Require
\State Initialize Parameters
\Repeat
	\State Read next record of file
	\State Lookup user and item keys in dictionary
	\State Update user and item feature vectors
	\State Update the total sum of errors variable
	
\Until {termination criteria is not reached}
\State Shuffle Data Set
\Ensure
\end{algorithmic}
\end{algorithm}

\paragraph{Notes}
\begin{itemize}

	\item Step Size: The algorithms performance is highly dependent on the step size (alpha) parameter of the update equations. Too small of a alpha will result in convergence taking a very long time and lower the chance of finding the global objective. Too large of a alpha will result in convergence not happening as the the algorithm continually 'overshoots' the optimal value. 
	
	\item Shuffle: When dealing with large data we need to be clever with how we shuffle our data. Shuffling is no trivial task out of memory. We explored several out of memory methods but ultimately decided with using a pseudo-shuffle, where we read in the file in large chunks that fit in memory, shuffle the lines using built in python functions, write the records back to a text file, and continue until the all of the file has been pseudo-shuffled. Hopefully by choosing a large enough batch size for reading in, our file is shuffle well enough. We also explored with a more robust method of shuffling the shuffled batches, but the added time complexity of appending large files together did not prove worthwhile given almost negligible returns in convergence.

\end{itemize}

\subsection*{Code Example}
For matrices that fit into memory use the \texttt{MatrixCompletion} module. Otherwise, use \texttt{MatrixCompletionBD} module. 

\subsubsection*{MatrixCompletion}
Given a \texttt{numpy} matrix $M$ with \texttt{numpy.nan} on the missing entries, the matrix completion problem can be solved (using ASD, for example) as:
\begin{verbatim}
>>> from completethat import MatrixCompletion
>>> problem = MatrixCompletion(M)
>>> problem.complete_it("ASD")
>>> X = problem.get_matrix() #Desired matrix
>>> out_info = problem.get_out() #Extra information
\end{verbatim}

\subsubsection*{MatrixCompletionBD}
Given a csv file with the input data, the matrix completion problem can be solved as:

\begin{verbatim}
>>> from completethat import MatrixCompletionBD
>>> problem = MatrixCompletionBD('input_data.txt')
>>> problem.train_sgd(dimension=6,init_step_size=.01,min_step=.000001, 
       reltol=.001,rand_init_scale=10, maxiter=1000,
       batch_size_sgd=50000,shuffle=True)
>>> problem.validate_sgd('test_data.txt')
>>> problem.save_model()
\end{verbatim}

\section{Case Studies}
\subsection*{Yahoo movies and music reviews database}
The Yahoo music and movie data sets are publicly available datasets published by Yahoo for use to the public for recreational and research purposes. The music training data set is a small 4 mb pipe-delimited file consisting of roughly 210,000 user-movie ratings. The test set has roughly 10,000 records. There are two ratings schemas available, one on a 5-point scale and the other on a 13-point scale and we arbitrarily chose to use the 5-point scale. The Yahoo music data is a larger 2.2 gb file of 115 million user-song ratings. The ratings are on a 100 point scale.
 
We look to evaluate the performance of our SGD algorithm on these two datasets comparing it versus a naive benchmark estimate, where we used the mean of the training set as our estimator for all records in the test set. Then we compared how well the algorithms performed versus the benchmark. 

The benchmark for the yahoo music data is 4.088. This is the mean value of the ratings in the training set so we will compute the MSE on the test set using the benchmark which yields an RMSE of 1.10. A quick run of Mahout yields 1.17 and CompleteThat's sgd model yielded a 1.19. So our model yielded a very similar RMSE as the MAHOUT package. Note that for both of these instances we used a factorization of rank equal to seven and the Complete that sgd algorithm ran slightly faster than Mahout's (.537 minutes vs 1.4 minutes). 

\begin{table}[h]
\centering
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\multicolumn{1}{l}{\textbf{Step size}} & \multicolumn{1}{l}{\textbf{Iterations}} & \multicolumn{1}{l}{\textbf{Minutes}} & \multicolumn{1}{l}{\textbf{Train MSE}} & \multicolumn{1}{l}{\textbf{Test RMSE}} \\ \midrule
1 & 8 & 0.585 & 1.411 & 1.757 \\
2 & 8 & 0.598 & 1.304 & 1.550 \\
3 & 8 & 0.628 & 1.235 & 1.483 \\
4 & 8 & 0.617 & 1.180 & 1.397 \\
5 & 7 & 0.538 & 1.136 & 1.351 \\
6 & 7 & 0.534 & 1.100 & 1.244 \\
7 & 7 & 0.534 & 1.071 & 1.194 \\
8 & 7 & 0.541 & 1.046 & 1.127 \\
9 & 7 & 0.537 & 1.027 & 1.128 \\
10 & 7 & 0.529 & 1.010 & 1.102 \\
15 & 7 & 0.551 & 0.971 & 1.063 \\
20 & 7 & 0.540 & 0.976 & 1.075 \\
30 & 8 & 0.614 & 1.045 & 1.359 \\ \bottomrule
\end{tabular}
}
\caption{RMSE vs Step Size vs Rank}
\end{table}

\clearpage
\subsection*{Columbia University photographs}
In this section, we demonstrate the applicability of the package to image processing problems. Grayscale pictures can be transform into a rectangular matrix where each element of the matrix determines the intensity of the corresponding pixel. For convenience, most of the current digital files use integer numbers between 0 and 255. By randomly erasing, or setting to zero, a predefined proportion of the pixels of an image the problem of completing the image can be cast as a matrix completion problem, where the missing entries are the those with zero on it. 

For illustration we used three  512 x 512 grayscale photographs taken by one of the authors of the Columbia University campus on December, 2014. The original, erased and reconstructed images are shown on Figure~\ref{fig:columbia_pics}, where we erased 65\% of the pixels of each image.  In addition, to illustrate the ease of use of the package, we also present the (very short) script used to recover the photographs:

\lstinputlisting[numbers=none]{columbia_images.py}

\begin{figure}[H]
\resizebox{\textwidth}{!}{%
\makebox[\textwidth][c]
{
\begin{tabular}{ccc}
\includegraphics[width=65mm]{figures/columbia1.png}
&
\includegraphics[width=65mm]{figures/columbia2.png}

&
\includegraphics[width=65mm]{figures/columbia3.png}

\\

\includegraphics[width=65mm]{figures/columbia4.png}
&
\includegraphics[width=65mm]{figures/columbia5.png}
&
\includegraphics[width=65mm]{figures/columbia6.png}
\\

\includegraphics[width=65mm]{figures/columbia7.png}
&
\includegraphics[width=65mm]{figures/columbia8.png}
&
\includegraphics[width=65mm]{figures/columbia9.png}
\\
\end{tabular}
}
}

\caption{ASD method applied three different Columbia University photographs using random sampling}
\label{fig:columbia_pics}
\end{figure}

\section{Future Work}

We have plans for various improvements as we continue working with the \texttt{CompleteThat} python package. Overall, we would like to test the software and integrate automated testing cases into the software developing cycle. Also, documentation for the package and its functions, including examples, is high in our priority list. In addition, we hope to incorporate more rigorous error checking and make   available more customization to the user. 

For the memory-based algorithms it is well know that noise in the data introduces overfitting on the estimated matrix. Following the approach taken by Mazumder et al.~\cite{mazumder:2010}  where they solve the same objetive function as the problems above (using the Frobenious norm) but introducing the nuclear norm as regularizer to account for overfitting, we would introduce a regularization option into the matrix completion procedure. 

For the stochastic gradient descent method, we plan to add a lambda penalty feature for combatting overfitting as well as considering options for more advanced versions of our basic latent factor model as demonstrated by Koren~\cite{koren:2008} in his paper on the prize-winning Netflix models. 
For all algorithms, we would like to explore and research various hardware and software optimization techniques for faster code. 

Finally, given the similarities between matrix completion and robust principal component analysis one further extension to the functionality of the package could be implementing the robust-PCA procedure outlined by Cand\'es~\cite{candes:2011}, et al.

\newpage
\bibliographystyle{alpha}
\bibliography{references}

\newpage
\section{Source code}
On the whole, the package directory structure looks like Figure~\ref{fig:code_scheme}. In what follows we present the source code of the package.
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{./figures/code_scheme.pdf}
    \caption{CompleteThat structure}
    \label{fig:code_scheme}
\end{figure}

\code{completethat/matrix\_completion.py}{../code/completethat/matrix_completion.py}
\code{completethat/matrix\_\_init\_\_.py}{../code/completethat/__init__.py}
\code{setup.py}{../code/setup.py}
\hrulefill
\subsection*{README.rst}
\lstinputlisting{../code/README.rst}

\end{document}